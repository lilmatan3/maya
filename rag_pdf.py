# rag_chat.py

from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
import os

# === 爪专转  砖 ===
model = OllamaLLM(model="aya")

# === 转转 砖-转砖 注专转 ===
template = """
住 专 :
{reviews}

 砖:
{question}

注 爪专 专专, 注专转, 注 住住 住 .
"""
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

# === 专转 住住 转 (Chroma) ===
db_location = "./chrome_langchain_db"
vector_store = Chroma(
    collection_name="hebrew_documents",
    persist_directory=db_location
)

# === 专专专 驻砖 住 ===
retriever = vector_store.as_retriever()

# === 砖  ===
while True:
    print("\n" + "-"*40)
    question = input("锔 砖 砖 (q 爪): ")
    if question.strip().lower() == "q":
        break

    # 砖驻转 住 专
    reviews = retriever.invoke(question)

    # 砖 
    result = chain.invoke({
        "reviews": reviews,
        "question": question
    })

    print("\n 转砖:", result)
